{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Analyse de sentiments dans les critiques de films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "1. Implémenter une manière simple de représenter des données textuelles\n",
    "2. Implémenter un modèle d'apprentissage statistique basique\n",
    "3. Utiliser ces représentations et ce modèle pour une tâche d'analyse de sentiments\n",
    "4. Tenter d'améliorer les résultats avec des outils venus du traitement automatique du langage\n",
    "5. Comparer les résultats avec une l'implémentation de Scikit-Learn, et avec d'autres méthodes de représentation ou d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dépendances nécessaires\n",
    "\n",
    "Pour les objectifs 4. et 5., on aura besoin des packages suivants:\n",
    "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
    "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
    "\n",
    "Les deux sont disponibles avec Anaconda: https://anaconda.org/anaconda/nltk et https://anaconda.org/anaconda/scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import re \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les données\n",
    "\n",
    "Extraire les données et placer le dossier 'data' dans le même dossier que le notebook.\n",
    "\n",
    "On récupère les données textuelles dans la variable *texts*\n",
    "\n",
    "On récupère les labels dans la variable $y$ qui en contient *len(texts)* : $0$ indique que la critique correspondante est négative tandis que $1$ qu'elle est positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "# We get the files from the path: ./data/imdb1/neg for negative reviews, and ./data/imdb1/pos for positive reviews\n",
    "filenames_neg = sorted(glob(op.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "# Each files contains a review that consists in one line of text: we put this string in two lists, that we concatenate\n",
    "texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n",
    "texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "\n",
    "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
    "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents: 200\n"
     ]
    }
   ],
   "source": [
    "# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n",
    "# Use an even number to keep the same number of positive and negative reviews\n",
    "k = 10\n",
    "texts_reduced = texts[0::k]\n",
    "y_reduced = y [0::k]\n",
    "\n",
    "print('Nombre de documents:', len(texts_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idée principale\n",
    "\n",
    "On dispose d'une critique étant en fait une liste de mots $s = (w_1, ..., w_N)$, et l'on cherche à trouver la classe associée $c$ - qui dans notre cas, peut-être $c = 0$ ou $c = 1$. L'objectif est donc de trouver pour chaque critique $s$ la classe $\\hat{c}$ maximisant la probabilité conditionelle **$P(c|s)$** : \n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)}$$\n",
    "\n",
    "**Hypothèse : P(s) est constante pour chaque classe** :\n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)} = \\underset{c}{\\mathrm{argmax}}\\,P(s|c)P(c)$$\n",
    "\n",
    "**Hypothèse naïve : les différentes variables (mots) d'une critique sont indépendantes entre elles** : \n",
    "\n",
    "$$P(s|c) = P(w_1, ..., w_N|c)=\\Pi_{i=1..N}P(w_i|c)$$\n",
    "\n",
    "On va donc pouvoir se servir des critiques annotées à notre disposition pour **estimer les probabilités $P(w|c)$ pour chaque mot $w$ étant donné les deux classes $c$**. Ces critiques vont nous permettre d'apprendre à évaluer la \"compatibilité\" entre les mots et classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vue générale\n",
    "\n",
    "### Entraînement: Estimer les probabilités\n",
    "\n",
    "Pour chaque mot $w$ du vocabulaire $V$, $P(w|c)$ est le nombre d'occurences de $w$ dans une critique ayant pour classe $c$, divisé par le nombre total d'occurences dans $c$. Si on note $T(w,c)$ ce nombre d'occurences, on obtient:\n",
    "\n",
    "$$P(w|c) = \\text{Fréquence de }w\\text{ dans }c = \\frac{T(w,c)}{\\sum_{w' \\in V} T(w',c)}$$\n",
    "\n",
    "### Test: Calcul des scores\n",
    "\n",
    "Pour faciliter les calculs et éviter les erreurs d'*underflow* et d'approximation, on utilise le \"log-sum trick\", et on passe l'équation en log-probabilités : \n",
    "\n",
    "$$\\hat{c} =  \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\, \\left[ \\mathrm{log}(P(c)) + \\sum_{i=1..N}log(P(w_i|c)) \\right]$$\n",
    "\n",
    "### Laplace smoothing (Lissage)\n",
    "\n",
    "Un mot qui n'apparaît pas dans un document a une probabilité nulle: cela va poser problème avec le logarithme. On garde donc une toute petite partie de la masse de probabilité qu'on redistribue avec le *Laplace smoothing*: \n",
    "\n",
    "$$P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} T(w',c) + 1}$$\n",
    "\n",
    "Il existe d'autre méthodes de lissage, en général adaptées à d'autres applications plus complexes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation adaptée des documents\n",
    "\n",
    "Notre modèle statistique, comme la plupart des modèles appliqués aux données textuelles, utilise les comptes d'occurences de mots dans un document. Ainsi, une manière très pratique de représenter un document est d'utiliser un vecteur \"Bag-of-Words\" (BoW), contenant les comptes de chaque mot (indifférement de leur ordre d'apparition) dans le document. \n",
    "\n",
    "Si on considère l'ensemble de tous les mots apparaissant dans nos $T$ documents d'apprentissage, que l'on note $V$ (Vocabulaire), on peut créer **un index**, qui est une bijection associant à chaque mot $w$ un entier, qui sera sa position dans $V$. \n",
    "\n",
    "Ainsi, pour un document extrait d'un ensemble de documents contenant $|V|$ mots différents, une représentation BoW sera un vecteur de taille $|V|$, dont la valeur à l'indice d'un mot $w$ sera son nombre d'occurences dans le document. \n",
    "\n",
    "On peut utiliser la classe **CountVectorizer** de scikit-learn pour mieux comprendre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avenue', 'boulevard', 'city', 'down', 'ran', 'the', 'walk', 'walked']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 2, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 2, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "Bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "Bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche d'abord la liste contenant les mots ordonnés selon leur indice (On note que les mots de 2 caractères ou moins ne sont pas pris en compte)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détail: entraînement\n",
    "\n",
    "L'idée est d'extraire le nombre d'occurences $T(w,c)$ de chaque mot $w$ pour chaque classe $c$, ce qui permettra de calculer la matrice de probabilités conditionelles $\\pmb{P}$ telle que: $$\\pmb{P}_{w,c} = P(w|c)$$\n",
    "\n",
    "Notons que le nombre d'occurences $T(w,c)$ peut être obtenu facilement à partir des représentations BoW de l'ensemble des documents.\n",
    "\n",
    "### Procédure:\n",
    "<img src=\"algo_train.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "## Détail: test\n",
    "\n",
    "Nous connaissons maintenant les probabilités conditionelles données par la matrice $\\pmb{P}$. \n",
    "Il faut maintenant obtenir $P(s|c)$ pour le document courant. Cette quantité s'obtient à l'aide d'un calcul simple impliquant la représentation BoW du document et $\\pmb{P}$.\n",
    "\n",
    "### Procédure:\n",
    "<img src=\"algo_test.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing du texte: obtenir les représentations BoW\n",
    "\n",
    "D'abord, il faut transformer les critiques sous forme de strings en une liste de mots. La tactique la plus simple consiste à diviser le string suivant les espaces, avec la commande:\n",
    "```text.split()```\n",
    "\n",
    "Mais il faut aussi faire attention à enlever les caractères particuliers qui pourraient ne pas avoir été nettoyés (comme les balises HTML si on a obtenu les données à partir de pages web). Puisque l'on va compter les mots, il faudra construire une liste des mots apparaissant dans nos données. Dans notre cas, on aimerait réduire cette liste et l'uniformiser (ignorer les majuscules, la ponctuation, et les mots les plus courts). \n",
    "On va donc utiliser une fonction adaptée à nos besoins - mais c'est un travail qu'il n'est en général pas nécessaire de faire, puisqu'il existe de nombreux outils déjà adaptés à la plupart des cas de figures. \n",
    "Pour le nettoyage du texte, il existe de nombreux scripts, basés sur différents outils (expressions régulières, par exemple) qui permettent de préparer des données. La division du texte en mots et la gestion de la ponctuation est gérée lors d'une étape appellée *tokenization*; si besoin, un package python comme le NLTK contient de nombreux *tokenizers* différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jean/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walked', 'down', 'down', 'the', 'boulevard', 'walked', 'down', 'the', 'avenue', 'ran', 'down', 'the', 'boulevard', 'walk', 'down', 'the', 'city', 'walk', 'down', 'the', 'the', 'avenue']\n",
      "['I', 'walked', 'down', 'down', 'the', 'boulevard', '.', 'I', 'walked', 'down', 'the', 'avenue', '.', 'I', 'ran', 'down', 'the', 'boulevard', '.', 'I', 'walk', 'down', 'the', 'city', '.', 'I', 'walk', 'down', 'the', 'the', 'avenue', '.']\n"
     ]
    }
   ],
   "source": [
    "# We might want to clean the file with various strategies:\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"        \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove small words (1 and 2 characters)\n",
    "    text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", text)\n",
    "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
    "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    text = REPLACE_HTML.sub(\" \", text)\n",
    "    \n",
    "    tokens = text.split()        \n",
    "    return tokens\n",
    "\n",
    "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
    "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus_raw = \"I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.\"\n",
    "print(clean_and_tokenize(corpus_raw))\n",
    "print(word_tokenize(corpus_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction **à compléter**. Elle prend en entrée une liste de document (chacun sous la forme d'un string) et renvoie, comme dans l'exemple utilisant ```CountVectorizer```:\n",
    "- Un vocabulaire qui associe à chaque mot rencontré un index\n",
    "- Une matrice, dont les lignes représentent les documents et les colonnes les mots indexés par le vocabulaire. En position $(i,j)$, on devra avoir le nombre d'occurence du mot $j$ dans le document $i$.\n",
    "\n",
    "Le vocabulaire, qui était sous la forme d'une *liste* dans l'exemple précédent, pourra être renvoyé sous forme de *dictionnaire* dont les clés sont les mots et les valeurs les indices. Puisque le vocabulaire recense les mots du corpus sans se soucier de leur nombre d'occurences, on pourra le constituer à l'aide d'un ensemble (```set``` en python). \n",
    "On pourra bien sur utiliser la fonction ```clean_and_tokenize``` pour transformer les strings en liste de mots. \n",
    "##### Quelques pointeurs pour les débutants en Python : \n",
    "\n",
    "- ```my_list.append(value)``` : put the variable ```value``` at the end of the list ```my_list```\n",
    "\n",
    "-  ```words = set()``` : create a set, which is a list of unique values \n",
    "\n",
    "- ```words.union(my_list)``` : extend the set ```words```\n",
    "\n",
    "- ```dict(zip(keys, values)))``` : create a dictionnary\n",
    "\n",
    "- ```for k, text in enumerate(texts)``` : syntax for a loop with the index, ```texts``` begin a list (of texts !)\n",
    "\n",
    "- ```len(my-list)``` : length of the list ```my_list```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I walked down down the boulevard',\n",
       " 'I walked down the avenue',\n",
       " 'I ran down the boulevard',\n",
       " 'I walk down the city',\n",
       " 'I walk down the the avenue']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    clean=[]\n",
    "    for text in texts:\n",
    "        clean.append(clean_and_tokenize(text))\n",
    "        \n",
    "    flatten = []\n",
    "    for sublist in clean:\n",
    "        for item in sublist:\n",
    "            flatten.append(item)\n",
    "    \n",
    "    myset = sorted(set(flatten))\n",
    "    \n",
    "    keys = range(0,len(myset))\n",
    "    \n",
    "    vocabulary = dict(zip(myset,keys))\n",
    "    \n",
    "    counts = np.zeros((len(clean),len(vocabulary)))            \n",
    "    \n",
    "    for k,text in enumerate(clean):\n",
    "        for word in text:\n",
    "            if word in vocabulary:\n",
    "                counts[k,vocabulary[word]] += 1\n",
    "                \n",
    "                \n",
    "    \n",
    "    return vocabulary,counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avenue': 0, 'boulevard': 1, 'city': 2, 'down': 3, 'ran': 4, 'the': 5, 'walk': 6, 'walked': 7}\n",
      "[[0. 1. 0. 2. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Voc,X= count_words(corpus)\n",
    "print(Voc)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes \n",
    "\n",
    "Classe vide : fonctions **à compléter** \n",
    "```python\n",
    "def fit(self, X, y)\n",
    "``` \n",
    "**Entraînement** : va apprendre un modèle statistique basés sur les représentations $X$ correspondant aux labels $y$.\n",
    "$X$ représente donc ici des représentations obtenues en sortie de count_words. On complète la fonction à l'aide de la procédure détaillée plus haut. Si il est possible de la suivre à la lettre, les représentations que l'on utilise nous permettre d'être bien plus efficace et d'éviter d'utiliser des boucles !\n",
    "\n",
    "\n",
    "Note: le lissage, effectué à la ligne $10$, ne se fait pas nécessairement avec un $1$, mais peut se faire avec une valeur positive $\\alpha$, qu'on peut implémenter comme argument de la classe ```NB```.\n",
    "\n",
    "```python\n",
    "def predict(self, X)\n",
    "```\n",
    "**Testing** : va renvoyer les labels prédits par le modèle pour d'autres représentations $X$.\n",
    "\n",
    "\n",
    "\n",
    "Pour faciliter la procédure, on prendra la moitié de la matrice $X$ obtenue plus haut pour entraîner le modèle, et l'autre moitié pour l'évaluer. **Important**: cette façon de procéder n'est pas réaliste: en général, on ne dispose que des données d'entraînement au moment de créer le vocabulaire et d'entraîner le modèle. Ainsi, il est possible que les données d'évaluation contiennent des mots *inconnus*. C'est quelque chose qu'on peut traiter facilement en dédiant un indice à tous les mots rencontrés qui ne sont pas contenus dans le vocabulaire - mais il existe de nombreuses méthodes plus complexes pour réussir à utiliser à bon escient ces mots que le modèle n'a pas rencontré à l'entraînement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quelques pointeurs pour les débutants en Python : \n",
    "\n",
    "Utiliser l'API Numpy pour travailler avec des tenseurs\n",
    "\n",
    "\n",
    "- ```X.shape``` : for a ```numpy.array```, return the dimension of the tensor\n",
    "\n",
    "- ```np.zeros((dim_1, dim_2,...))``` : create a tensor filled with zeros\n",
    "\n",
    "- ```np.sum(X, axis = n)``` : sum the tensor over the axis n\n",
    "\n",
    "- ```np.mean(X, axis = n)```\n",
    "\n",
    "- ```np.argmax(X, axis = n)```\n",
    "\n",
    "- ```np.log(X)```\n",
    "\n",
    "- ```np.dot(X_1, X_1)``` : Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    # Les arguments de classe permettent l'héritage de classes de sklearn\n",
    "    def __init__(self, alpha=1.0):\n",
    "        # alpha est un paramètre pour le lissage: il correspond à la valeur ligne 10 de l'algorithme d'entraînement\n",
    "        # Dans l'algorithme d'entraînement, et comme valeur par défaut, on utilise alpha = 1\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # sort data into classes\n",
    "        Xy0 = X[y == 0]\n",
    "        Xy1 = X[y == 1]\n",
    "        \n",
    "        # calculate priors\n",
    "        priory0 = len(Xy0) / len(X)\n",
    "        priory1 = len(Xy1) / len(X)\n",
    "        \n",
    "        self.prior = np.array([priory0,priory1])\n",
    "        \n",
    "        cond_prob = np.zeros((X.shape[1],2))\n",
    "        \n",
    "        for i in range (len(X[0])):\n",
    "            cond_prob[i,0] = (np.sum(Xy0[:,i])+1)/(np.sum(Xy0)+len(X[0]))\n",
    "            cond_prob[i,1] = (np.sum(Xy1[:,i])+1)/(np.sum(Xy1)+len(X[0]))\n",
    "              \n",
    "        self.cond = cond_prob\n",
    "                                                   \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        score = np.zeros((2,X.shape[0]))\n",
    "        \n",
    "        \n",
    "        score[0,:] = np.log(self.prior[0])\n",
    "        score[0,:] += X @ np.log(self.cond[:,0])\n",
    "        \n",
    "        score[1,:] = np.log(self.prior[1])\n",
    "        score[1,:] += X @ np.log(self.cond[:,1])\n",
    "\n",
    "        result = np.argmax(score,axis=0)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expérimentation\n",
    "\n",
    "On utilise la moitié des données pour l'entraînement, l'autre pour tester le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, X = count_words(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(41354, 2)\n",
      "0.821\n"
     ]
    }
   ],
   "source": [
    "nb = NB()\n",
    "nb.fit(X[::2], y[::2])\n",
    "print(nb.prior.shape)\n",
    "print(nb.cond.shape)\n",
    "print(nb.score(X[1::2], y[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation \n",
    "\n",
    "Avec la fonction *cross_val_score* de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.8099999999999999 (std 0.010839741694339374)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(nb, X, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer les performances: \n",
    "\n",
    "**Quelles sont les points forts et les points faibles de ce système ? Comment y remédier ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour aller plus loin: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Améliorer les représentations\n",
    "\n",
    "On utilise la function \n",
    "```python\n",
    "CountVectorizer\n",
    "``` \n",
    "de scikit-learn pour constituer notre corpus. Elle nous permettra d'améliorer facilement nos représentations BoW.\n",
    "\n",
    "#### Tf-idf:\n",
    "\n",
    "Il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF).\n",
    "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice d'occurences $mots \\times documents$. Ainsi, pour une matrice $\\mathbf{T}$ de $|V|$ termes et $D$ documents:\n",
    "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n",
    "\n",
    "$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
    "\n",
    "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. \n",
    "    \n",
    "#### Ne pas prendre en compte les mots trop fréquents:\n",
    "\n",
    "On peut utiliser l'option \n",
    "```python\n",
    "max_df=1.0\n",
    "```\n",
    "pour modifier la quantité de mots pris en compte. \n",
    "\n",
    "#### Essayer différentes granularités:\n",
    "\n",
    "Plutôt que de simplement compter les mots, on peut compter les séquences de mots - de taille limitée, bien sur. \n",
    "On appelle une séquence de $n$ mots un $n$-gram: essayons d'utiliser les 2 et 3-grams (bi- et trigrams).\n",
    "On peut aussi tenter d'utiliser les séquences de caractères à la place de séquences de mots.\n",
    "\n",
    "On s'intéressera aux options \n",
    "```python\n",
    "analyzer='word'\n",
    "```\n",
    "et \n",
    "```python\n",
    "ngram_range=(1, 2)\n",
    "```\n",
    "que l'on changera pour modifier la granularité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: %s (std %s) (0.812, 0.012884098726725092)\n",
      "Classification score tf-idf: %s (std %s) (0.8125, 0.010488088481701477)\n",
      "Classification score sans mots fréquents: %s (std %s) (0.812, 0.012884098726725092)\n",
      "Classification score bigram: %s (std %s) (0.8305, 0.009273618495495711)\n",
      "Classification score trigram: %s (std %s) (0.8225, 0.017320508075688742)\n",
      "Classification score char: %s (std %s) (0.6094999999999999, 0.018261982367749664)\n"
     ]
    }
   ],
   "source": [
    "## On peut définir une pipeline que l'on modifiera pour expérimenter.\n",
    "\n",
    "pipeline_base = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_base, texts, y, cv=5)\n",
    "print(\"Classification score: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_tf_idf = Pipeline([\n",
    "        ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
    "        ('tfid', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB())\n",
    "])\n",
    "scores = cross_val_score(pipeline_tf_idf, texts, y, cv=5)\n",
    "print(\"Classification score tf-idf: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_maxdf = Pipeline([\n",
    "        ('vect', CountVectorizer(analyzer='word', stop_words=None,max_df=1.0)),\n",
    "        ('clf', MultinomialNB())\n",
    "        \n",
    "])\n",
    "scores = cross_val_score(pipeline_maxdf, texts, y, cv=5)\n",
    "print(\"Classification score sans mots fréquents: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_bigram = Pipeline([\n",
    "        ('vect', CountVectorizer(analyzer='word', stop_words=None,ngram_range=(1, 2))),\n",
    "        ('clf', MultinomialNB())\n",
    "        \n",
    "])\n",
    "scores = cross_val_score(pipeline_bigram, texts, y, cv=5)\n",
    "print(\"Classification score bigram: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_trigram = Pipeline([\n",
    "        ('vect', CountVectorizer(analyzer='word', stop_words=None,ngram_range=(1, 3))),\n",
    "        ('clf', MultinomialNB())\n",
    "        \n",
    "])\n",
    "scores = cross_val_score(pipeline_trigram, texts, y, cv=5)\n",
    "print(\"Classification score trigram: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_char = Pipeline([\n",
    "        ('vect', CountVectorizer(analyzer='char', stop_words=None)),\n",
    "        ('clf', MultinomialNB())\n",
    " \n",
    "])\n",
    "scores = cross_val_score(pipeline_char, texts, y, cv=5)\n",
    "print(\"Classification score char: %s (std %s)\",(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "\n",
    "Permet de revenir à la racine d'un mot: on peut ainsi grouper différents mots autour de la même racine, ce qui facilite la généralisation. Utiliser:\n",
    "```python\n",
    "from nltk import SnowballStemmer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word : singers ; stemmed : singer\n",
      "word : cat ; stemmed : cat\n",
      "word : generalization ; stemmed : general\n",
      "word : philosophy ; stemmed : philosophi\n",
      "word : psychology ; stemmed : psycholog\n",
      "word : philosopher ; stemmed : philosoph\n"
     ]
    }
   ],
   "source": [
    "words = ['singers', 'cat', 'generalization', 'philosophy', 'psychology', 'philosopher']\n",
    "for word in words:\n",
    "    print('word : %s ; stemmed : %s' %(word, stemmer.stem(word)))#.decode('utf-8'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation des données:\n",
    "\n",
    "Classe vide : function **à compléter** \n",
    "```python\n",
    "def stem(X)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(X): \n",
    "    X_stem = []\n",
    "    for word in X:\n",
    "        X_stem.append(stemmer.stem(word))\n",
    "    return X_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.8099999999999999 (std 0.010839741694339374)\n"
     ]
    }
   ],
   "source": [
    "texts_stemmed = stem(texts)\n",
    "voc, X = count_words(texts_stemmed)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie du discours\n",
    "\n",
    "Pour généraliser, on peut aussi utiliser les parties du discours (Part of Speech, POS) des mots, ce qui nous permettra  de filtrer l'information qui n'est potentiellement pas utile au modèle. On va récupérer les POS des mots à l'aide des fonctions:\n",
    "```python\n",
    "from nltk import pos_tag, word_tokenize\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jean/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('am', 'VBP'), ('Sam', 'NNP')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tag(word_tokenize(('I am Sam')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Détails des significations des tags POS: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Transformation des données:\n",
    "\n",
    "Classe vide : fonction **à compléter** \n",
    "```python\n",
    "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB'])\n",
    "``` \n",
    "\n",
    "Ne garder que les noms, adverbes, verbes et adjectifs pour notre modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB']):\n",
    "    X_pos = []\n",
    "    for words in X:\n",
    "        text=[]\n",
    "        tokens = word_tokenize(words)\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        good = [t for t in tags if t[1] in good_tags]\n",
    "        for i in range (len(good)):\n",
    "             text.append(good[i][0])  \n",
    "        X_pos.append(text)\n",
    "        \n",
    "    for j in range (len(X_pos)):\n",
    "        X_pos[j] = ' '.join(X_pos[j])\n",
    "        \n",
    "    return X_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.7825000000000001 (std 0.015411035007422467)\n"
     ]
    }
   ],
   "source": [
    "texts_POS = pos_tag_filter(texts)\n",
    "voc, X = count_words(texts_POS)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-words \n",
    "\n",
    "Les \"stop-words\" sont les mots apparaissant fréquemment dans les données et que l'on juge non représentatifs. On les considère comme du bruit. Une liste de stop-words est disponible dans le fichier *english.stop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readFile(fileName):\n",
    "    \"\"\"\n",
    "     * Code for reading a file.  you probably don't want to modify anything here, \n",
    "     * unless you don't like the way we segment files.\n",
    "    \"\"\"\n",
    "    contents = []\n",
    "    f = open(fileName)\n",
    "    for line in f:\n",
    "        contents.append(line)\n",
    "    f.close()\n",
    "    result = ('\\n'.join(contents)).split() \n",
    "    return result\n",
    "\n",
    "sw = readFile('english.stop')\n",
    "sw[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Transformation des données:\n",
    "\n",
    "Classe vide : fonction **à compléter** \n",
    "```python\n",
    "def filterStopWords(X)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterStopWords(X):\n",
    "    \"\"\"Filters stop words.\"\"\"\n",
    "    X_filtered = []\n",
    "    for words in X:\n",
    "        text=[]\n",
    "        tokens = word_tokenize(words)\n",
    "        good = [t for t in tokens if t not in sw]\n",
    "        for i in range (len(good)):\n",
    "             text.append(good[i])  \n",
    "        X_filtered.append(text)\n",
    "    \n",
    "    for j in range (len(X_filtered)):\n",
    "        X_filtered[j] = ' '.join(X_filtered[j])\n",
    "        \n",
    "    return X_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.8035 (std 0.01617096162879622)\n"
     ]
    }
   ],
   "source": [
    "texts_stop = filterStopWords(texts)\n",
    "voc, X = count_words(texts_stop)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Utilisation d'un classifieur plus complexe ?\n",
    "\n",
    "On peut utiliser les implémentations scikit-learn de classifieurs moins naïfs, comme la régression logistique ou les SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 0.8545 (std 0.00509901951359279)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jean/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 0.8210000000000001 (std 0.004062019202317978)\n"
     ]
    }
   ],
   "source": [
    "pipeline_logistic = Pipeline([\n",
    "        ('vect', CountVectorizer(analyzer='word', stop_words=None,max_df=1.0)),\n",
    "        ('tfid', TfidfTransformer()),\n",
    "        ('clf', LinearSVC())\n",
    "])\n",
    "scores = cross_val_score(pipeline_logistic, texts, y, cv=5)\n",
    "print(\"Classification score: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_svm = Pipeline([\n",
    "        ('vect', CountVectorizer(analyzer='word', stop_words=None,max_df=1.0)),\n",
    "        ('tfid', TfidfTransformer()),\n",
    "        ('clf', LogisticRegression())\n",
    "])\n",
    "scores = cross_val_score(pipeline_svm, texts, y, cv=5)\n",
    "print(\"Classification score: %s (std %s)\" % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "### Results on the all corpus\n",
    "\n",
    "#### Naive Bayes:\n",
    "Score de classification: 0.8099999999999999 (std 0.010839741694339374)\n",
    "\n",
    "This could be seen as our baseline result.\n",
    "\n",
    "#### Pipelines:\n",
    "\n",
    "**classic:**\n",
    "Classification score: %s (std %s) (0.812, 0.012884098726725092)\n",
    "\n",
    "**tf-idf**\n",
    "Classification score tf-idf: %s (std %s) (0.8125, 0.010488088481701477)\n",
    "\n",
    "**sans mots fréquents**\n",
    "Classification score sans mots fréquents: %s (std %s) (0.812, 0.012884098726725092)\n",
    "\n",
    "**bigram**\n",
    "Classification score bigram: %s (std %s) (0.8305, 0.009273618495495711)\n",
    "\n",
    "**trigram**\n",
    "Classification score trigram: %s (std %s) (0.8225, 0.017320508075688742)\n",
    "\n",
    "**char**\n",
    "Classification score char: %s (std %s) (0.6094999999999999, 0.018261982367749664)\n",
    "\n",
    "We can see an improvement using the basic pipeline which is even better when we are getting rid of the most frequent words, seen as noise. \n",
    "\n",
    "The ponderation method \"tf-idf\" also help us to improve the scrore by having a better understanding of the corpus by assigning higher weights to the most frequent and relevant words. \n",
    "\n",
    "Using bigram or trigran seems to give higher result, this could be because it helped the program to have a better understanding of what the corpus is really about. Indeed, some words can have different meaning while being associated with other. *for example : If I take the word \"extraordinary\" alone, it might be seen as a positive word. However, if we have the association \"extraordinary lame\" it is in fact negative*. \n",
    "Also, taking too long associations might lead to overfitting because those associations will occur only in our training example.\n",
    "\n",
    "Using character based classifier also seems a way less effective which is quite logical since the meaning of things and the way languages are build is more based on words.\n",
    "\n",
    "#### Stemming:\n",
    "\n",
    "Score de classification: 0.8099999999999999 (std 0.010839741694339374)\n",
    "\n",
    "Stemming seems to give exactly the same result as the baseline, this could be explain by how it works. Indeed here we are associating each word to a root, it seems that we are not adding or taking off any information.\n",
    "\n",
    "#### Part of speach:\n",
    "\n",
    "Score de classification: 0.7825000000000001 (std 0.015411035007422467)\n",
    "\n",
    "Using Part Of Speech technique, we are filtering the corpus to keep only the relevant information. It seems we filtered the corpus a bit too much here because we lost some information which leads to lower performance. \n",
    "\n",
    "#### Stop words:\n",
    "\n",
    "Score de classification: 0.8035 (std 0.01617096162879622)\n",
    "\n",
    "Same as before, lower score means we lost a bit of info. Here we probably get rid of too much words (stop words) while they were useful to get the meaning of the text. \n",
    "\n",
    "#### More complex classifier:\n",
    "\n",
    "**LinearSVC**\n",
    "Classification score: 0.8545 (std 0.00509901951359279)\n",
    "\n",
    "**LogisticRegression**\n",
    "Classification score: 0.8210000000000001 (std 0.004062019202317978)\n",
    "\n",
    "Using more complex classifiers seems to help with the performances. By doing that, we are allowed to fit better the data (ie the corpus). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
